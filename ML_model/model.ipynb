{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference websites:\n",
    "* https://www.hackster.io/news/easy-tinyml-on-esp32-and-arduino-a9dbc509f26c\n",
    "* https://github.com/eloquentarduino/EloquentTinyML\n",
    "* https://github.com/atomic14/tensorflow-lite-esp32\n",
    "* https://github.com/eloquentarduino/tinymlgen\n",
    "* https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization\n",
    "* https://medium.com/mlearning-ai/optimizing-tflite-models-for-on-edge-machine-learning-for-efficiency-a-comparison-of-quantization-2c0123959cb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code below is used to generate the FFT TFLite Model \n",
    "* Healthy data: Own dataset\n",
    "* Unhealthy data: Online dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jared\\Desktop\\EG3301R_Repos\\TFLite\\ML_model\\model.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jared/Desktop/EG3301R_Repos/TFLite/ML_model/model.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data_n\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jared/Desktop/EG3301R_Repos/TFLite/ML_model/model.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m raw_data_norm \u001b[39m=\u001b[39m dataReader(normal_filename, col_indices)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jared/Desktop/EG3301R_Repos/TFLite/ML_model/model.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m raw_data_norm\u001b[39m.\u001b[39miloc[:, [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]] \u001b[39m=\u001b[39m raw_data_norm\u001b[39m.\u001b[39;49miloc[:, [\u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m]] \u001b[39m# Swap columns for radial and tangential data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jared/Desktop/EG3301R_Repos/TFLite/ML_model/model.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m raw_data_norm \u001b[39m=\u001b[39m raw_data_norm \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m# Convert to g\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jared/Desktop/EG3301R_Repos/TFLite/ML_model/model.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m raw_data_imbalance \u001b[39m=\u001b[39m dataReader(imbalance_data, col_indices)\n",
      "File \u001b[1;32mc:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1096\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1098\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1594\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[1;32m-> 1594\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[0;32m   1595\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1596\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[1;32mc:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:904\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[0;32m    903\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 904\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(k, i)\n\u001b[0;32m    905\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    906\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    907\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLocation based indexing can only have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    908\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m] types\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1516\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1514\u001b[0m     \u001b[39m# check that the key does not exceed the maximum size of the index\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(arr) \u001b[39mand\u001b[39;00m (arr\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m arr\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis):\n\u001b[1;32m-> 1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1518\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan only index by location with a [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Read in only the vibration data from the Excel spreadsheet\n",
    "col_indices = [1,2,3]\n",
    "\n",
    "# Load the Excel spreadsheet\n",
    "normal_filename = glob.glob(\"./Own_data/*.csv\")\n",
    "imbalance_data = glob.glob(\"./Online_data/Machinary_Fault_dataset/imbalance/imbalance/35g/*.csv\")\n",
    "\n",
    "def dataReader(path_names, col_indices):\n",
    "    data_n = pd.DataFrame()\n",
    "    for i in path_names:\n",
    "        # Read only columns 0 to 6 which contains rotational frequency (1 column) + vibration data (6 columns)\n",
    "        low_data = pd.read_csv(i,header=None,usecols=col_indices) \n",
    "        data_n = pd.concat([data_n,low_data],ignore_index=True)\n",
    "    return data_n\n",
    "\n",
    "\n",
    "raw_data_norm = dataReader(normal_filename, col_indices)\n",
    "raw_data_norm.iloc[:, [1, 2]] = raw_data_norm.iloc[:, [2, 1]] # Swap columns for radial and tangential data\n",
    "raw_data_norm = raw_data_norm / 1000 # Convert to g\n",
    "\n",
    "raw_data_imbalance = dataReader(imbalance_data, col_indices)\n",
    "\n",
    "# Normalise the data\n",
    "def normalise(df):\n",
    "    df_normalized = df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    return df_normalized\n",
    "\n",
    "# Testing without normalisation\n",
    "# data_norm = raw_data_norm\n",
    "# data_imbalance = raw_data_imbalance\n",
    "\n",
    "# Testing with normalisation\n",
    "data_norm = normalise(raw_data_norm)\n",
    "data_imbalance = normalise(raw_data_imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checking if data is loaded in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_norm.info())\n",
    "print(data_imbalance.info())\n",
    "\n",
    "print(data_norm.head())\n",
    "print(data_imbalance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Downsampling to reduce size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampler(data, a, b):\n",
    "    \"\"\"\n",
    "    data = data\n",
    "    a = start index\n",
    "    b = sampling rate\n",
    "    \"\"\"\n",
    "    x = b\n",
    "    downsampled_data = [data.iloc[a:b,:].sum()/x for i in range(int(len(data)/x))]\n",
    "    return pd.DataFrame(downsampled_data)\n",
    "\n",
    "# Create donwsampled datasets for excluding microphone data\n",
    "ds_data_norm = downSampler(data_norm, 0, 2500)\n",
    "ds_data_imbalance = downSampler(data_imbalance, 0, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Checking that data is downsampled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_data_norm.shape)\n",
    "print(ds_data_imbalance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data processing. FFTConolve method is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def FFTConvolve(data):\n",
    "    autocorr = signal.fftconvolve(data,data[::-1],mode='full')\n",
    "    return pd.DataFrame(autocorr)\n",
    "\n",
    "# Create FFTConvolved datasets for excluding microphone data\n",
    "ds_data_norm_fftconvole = FFTConvolve(ds_data_norm)\n",
    "ds_data_imbalance_fftconvole = FFTConvolve(ds_data_imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Checking that the data processing step is done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_data_norm_fftconvole.shape) # Check if data is FFTConvolved correctly\n",
    "print(ds_data_imbalance_fftconvole.shape) # Check if data is FFTConvolved correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up labels for both datasets\n",
    "y_0 = pd.DataFrame(np.zeros(int(len(ds_data_norm_fftconvole)),dtype=int))\n",
    "y_1 = pd.DataFrame(np.ones(int(len(ds_data_imbalance_fftconvole)),dtype=int))\n",
    "y = pd.concat([y_0,y_1],axis=0)\n",
    "y # Check if labels are set correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Preparing data to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = pd.concat([ds_data_norm_fftconvole, ds_data_imbalance_fftconvole], ignore_index=True) # Concatenate all the data\n",
    "data_x # Check if data is concatenated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "# Needed for quantization, used to calibrate the range of all floating-point tensors in the model\n",
    "# batch needs to be 1 in order to work, 1000 samples are enough to calibrate the range\n",
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(1000):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(5,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax')) # Output layer needs to correspond to the number of classes for softmax\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "FFTmodel = get_model()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"FFT_model_quantized.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Converting of model to C array (Run below line on bash)\n",
    "\n",
    "`xxd -i FFT_model_dynamic_quantized.tflite > FFT_model_dynamic_quantized.cc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the multiple models using different quantization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tinymlgen import port\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(1000):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # Initialising ANN model for 2 columns\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(5,)))\n",
    "    # model.add(Dense(64, activation='relu',kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # model.add(Dense(64, activation='relu',kernel_initializer='random_uniform'))\n",
    "    # model.add(Dense(32, activation='relu',kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "FFTmodel = get_model()\n",
    "\n",
    "# Dyanmic range quantization\n",
    "dyanmic_converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel)\n",
    "dyanmic_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "dyanmic_tflite_model = dyanmic_converter.convert()\n",
    "\n",
    "# Float16 quantization\n",
    "float16_converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel)\n",
    "float16_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "float16_converter.target_spec.supported_types = [tf.float16]\n",
    "float16_tflite_model = float16_converter.convert()\n",
    "\n",
    "# Full integer quantization\n",
    "# Only this one able to load on the ESP32 currently !!\n",
    "fullint_converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel)\n",
    "fullint_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "fullint_converter.representative_dataset = representative_dataset\n",
    "fullint_tflite_model = fullint_converter.convert()\n",
    "\n",
    "# Save the models to disk\n",
    "open(\"FFT_model_dynamic_quantized.tflite\", \"wb\").write(dyanmic_tflite_model)\n",
    "open(\"FFT_model_float16_quantized.tflite\", \"wb\").write(float16_tflite_model)\n",
    "open(\"FFT_model_fullint_quantized.tflite\", \"wb\").write(fullint_tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the models built using different quantization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the baseline model on the test dataset.\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "_, baseline_model_accuracy = FFTmodel.evaluate(x=X_test, y=y_test, verbose=0)\n",
    "\n",
    "# Printing the baseline test accuracy in percentage.\n",
    "print('The Baseline test accuracy:', baseline_model_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    num_test_samples = len(x_test)\n",
    "\n",
    "    # Run predictions on every set in the \"test\" dataset.\n",
    "    prediction_y = []\n",
    "    for i in range(num_test_samples):\n",
    "\n",
    "        # Pre-processing the data to fit it with the model's input.\n",
    "        input_data = np.array(x_test.iloc[i,:], dtype=np.float32)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        prediction_y.append(output_data.argmax())\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_y)):\n",
    "        if prediction_y[index] == y_test.iloc[index][0]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_y)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Passing the FP-16 TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('FFT_model_float16_quantized.tflite')\n",
    "\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy_fp_16 = evaluate_model(interpreter)\n",
    "\n",
    "# Printing the test accuracy for the FP-16 quantized TFLite model and the baseline Keras model.\n",
    "print('Float 16 Quantized TFLite Model Test Accuracy:', test_accuracy_fp_16*100)\n",
    "\n",
    "# Testing the full integer quantized model on the test dataset.\n",
    "\n",
    "# Passing the full integer quantized TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('FFT_model_fullint_quantized.tflite')\n",
    "\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy_int = evaluate_model(interpreter)\n",
    "\n",
    "# Printing the test accuracy for the full integer quantized TFLite model and the baseline Keras model.\n",
    "print('Full Integer Quantized TFLite Model Test Accuracy:', test_accuracy_int*100)\n",
    "\n",
    "# Testing the dynamic quantized model on the test dataset.\n",
    "\n",
    "# Passing the dynamic quantized TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('FFT_model_dynamic_quantized.tflite')\n",
    "\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy_dynamic = evaluate_model(interpreter)\n",
    "\n",
    "# Printing the test accuracy for the dynamic quantized TFLite model and the baseline Keras model.\n",
    "print('Dynamic Quantized TFLite Model Test Accuracy:', test_accuracy_dynamic*100)\n",
    "\n",
    "# Printing the test accuracy for the baseline Keras model.\n",
    "print('Baseline Keras Model Test Accuracy:', baseline_model_accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
