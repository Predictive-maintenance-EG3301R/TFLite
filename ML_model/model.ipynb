{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference websites:\n",
    "* https://www.hackster.io/news/easy-tinyml-on-esp32-and-arduino-a9dbc509f26c\n",
    "* https://github.com/eloquentarduino/EloquentTinyML\n",
    "* https://github.com/atomic14/tensorflow-lite-esp32\n",
    "* https://github.com/eloquentarduino/tinymlgen\n",
    "* https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code below is used to generate the FFT TFLite Model \n",
    "* Healthy data: Own dataset\n",
    "* Unhealthy data: Online dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Read in only the vibration data from the Excel spreadsheet\n",
    "col_indices = [1,2,3]\n",
    "\n",
    "# Load the Excel spreadsheet\n",
    "normal_filename = glob.glob(\"./Own_data/*.csv\")\n",
    "imbalance_data = glob.glob(\"./Online_data/Machinary_Fault_dataset/imbalance/imbalance/35g/*.csv\")\n",
    "\n",
    "def dataReader(path_names, col_indices):\n",
    "    data_n = pd.DataFrame()\n",
    "    for i in path_names:\n",
    "        # Read only columns 0 to 6 which contains rotational frequency (1 column) + vibration data (6 columns)\n",
    "        low_data = pd.read_csv(i,header=None,usecols=col_indices) \n",
    "        data_n = pd.concat([data_n,low_data],ignore_index=True)\n",
    "    return data_n\n",
    "\n",
    "\n",
    "raw_data_norm = dataReader(normal_filename, col_indices)\n",
    "raw_data_norm.iloc[:, [1, 2]] = raw_data_norm.iloc[:, [2, 1]] # Swap columns for radial and tangential data\n",
    "raw_data_norm = raw_data_norm / 1000 # Convert to g\n",
    "\n",
    "raw_data_imbalance = dataReader(imbalance_data, col_indices)\n",
    "\n",
    "# Normalise the data\n",
    "def normalise(df):\n",
    "    df_normalized = df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    return df_normalized\n",
    "\n",
    "# Testing without normalisation\n",
    "# data_norm = raw_data_norm\n",
    "# data_imbalance = raw_data_imbalance\n",
    "\n",
    "# Testing with normalisation\n",
    "data_norm = normalise(raw_data_norm)\n",
    "data_imbalance = normalise(raw_data_imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checking if data is loaded in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_norm.info())\n",
    "print(data_imbalance.info())\n",
    "\n",
    "print(data_norm.head())\n",
    "print(data_imbalance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Downsampling to reduce size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampler(data, a, b):\n",
    "    \"\"\"\n",
    "    data = data\n",
    "    a = start index\n",
    "    b = sampling rate\n",
    "    \"\"\"\n",
    "    x = b\n",
    "    downsampled_data = [data.iloc[a:b,:].sum()/x for i in range(int(len(data)/x))]\n",
    "    return pd.DataFrame(downsampled_data)\n",
    "\n",
    "# Create donwsampled datasets for excluding microphone data\n",
    "ds_data_norm = downSampler(data_norm, 0, 2500)\n",
    "ds_data_imbalance = downSampler(data_imbalance, 0, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Checking that data is downsampled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_data_norm.shape)\n",
    "print(ds_data_imbalance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data processing. FFTConolve method is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def FFTConvolve(data):\n",
    "    autocorr = signal.fftconvolve(data,data[::-1],mode='full')\n",
    "    return pd.DataFrame(autocorr)\n",
    "\n",
    "# Create FFTConvolved datasets for excluding microphone data\n",
    "ds_data_norm_fftconvole = FFTConvolve(ds_data_norm)\n",
    "ds_data_imbalance_fftconvole = FFTConvolve(ds_data_imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Checking that the data processing step is done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_data_norm_fftconvole.shape) # Check if data is FFTConvolved correctly\n",
    "print(ds_data_imbalance_fftconvole.shape) # Check if data is FFTConvolved correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up labels for both datasets\n",
    "y_0 = pd.DataFrame(np.zeros(int(len(ds_data_norm_fftconvole)),dtype=int))\n",
    "y_1 = pd.DataFrame(np.ones(int(len(ds_data_imbalance_fftconvole)),dtype=int))\n",
    "y = pd.concat([y_0,y_1],axis=0)\n",
    "y # Check if labels are set correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Preparing data to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = pd.concat([ds_data_norm_fftconvole, ds_data_imbalance_fftconvole], ignore_index=True) # Concatenate all the data\n",
    "data_x # Check if data is concatenated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "# Needed for quantization, used to calibrate the range of all floating-point tensors in the model\n",
    "# batch needs to be 1 in order to work, 1000 samples are enough to calibrate the range\n",
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(1000):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(5,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax')) # Output layer needs to correspond to the number of classes for softmax\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "FFTmodel = get_model()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"FFT_model_quantized.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Converting of model to C array (Run below line on bash)\n",
    "\n",
    "`xxd -i FFT_model_quantized.tflite > FFT_model_quantized.cc`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
