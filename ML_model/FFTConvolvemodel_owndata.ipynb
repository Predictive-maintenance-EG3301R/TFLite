{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference websites:\n",
    "* https://www.hackster.io/news/easy-tinyml-on-esp32-and-arduino-a9dbc509f26c\n",
    "* https://github.com/eloquentarduino/EloquentTinyML\n",
    "* https://github.com/atomic14/tensorflow-lite-esp32\n",
    "* https://github.com/eloquentarduino/tinymlgen\n",
    "* https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization\n",
    "* https://medium.com/mlearning-ai/optimizing-tflite-models-for-on-edge-machine-learning-for-efficiency-a-comparison-of-quantization-2c0123959cb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to check out:\n",
    "* https://www.tensorflow.org/model_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code below is used to generate the FFT TFLite Model \n",
    "* Healthy data: Own dataset\n",
    "* Unhealthy data: Online dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t00-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t00-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t01-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t01-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t02-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t02-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t03-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t03-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t04-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t04-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t05-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t05-30-01.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t06-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t06-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t07-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t07-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t08-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t08-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t09-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned\\OneKhz2023-03-24t09-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t00-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t00-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t01-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t01-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t02-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t02-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t03-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t03-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t04-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t04-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t05-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t05-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t06-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t06-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t07-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t07-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t08-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t08-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t09-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned\\OneKhz2023-03-24t09-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t00-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t00-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t01-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t01-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t02-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t02-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t03-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t03-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t04-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t04-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t05-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t05-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t06-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t06-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t07-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t07-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t08-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t08-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t09-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned\\OneKhz2023-03-23t09-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t00-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t00-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t01-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t01-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t02-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t02-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t03-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t03-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t04-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t04-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t05-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t05-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t06-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t06-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t07-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t07-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t08-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t08-30-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t09-00-00.csv\n",
      "done with file: C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned\\OneKhz2023-03-23t09-30-00.csv\n",
      "starting size: \n",
      "(4917380, 3)\n",
      "(4900690, 3)\n",
      "(4940044, 3)\n",
      "(4943184, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Selecting the relevant data\n",
    "\n",
    "vibe_col = [1,2,3]\n",
    "healthy_vert = 'C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/vertical-cleaned/OneKhz2023-03-24t0*.csv'\n",
    "healthy_hori = 'C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/healthy/second-batch-27-3-2023-cleaned/horizontal-cleaned/OneKhz2023-03-24t0*.csv'\n",
    "unhealthy_vert = 'C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/vertical-cleaned/OneKhz2023-03-23t0*.csv'\n",
    "unhealthy_hori = 'C:/Users/jared/OneDrive - National University of Singapore/Y2/S2/EG3301R/DataCollection/unhealthy/loose-base/first-batch-22-3-2023/vibration/horizontal-cleaned/OneKhz2023-03-23t0*.csv'\n",
    "\n",
    "# Loading the data\n",
    "def dataReader(datapath):\n",
    "    data_n = pd.DataFrame()\n",
    "    for file in glob.glob(datapath):\n",
    "        df = pd.read_csv(file, usecols=['x', 'y', 'z'])\n",
    "\n",
    "        data_n = pd.concat([data_n, df], axis=0)\n",
    "        print(\"done with file: \" + file)\n",
    "\n",
    "    return data_n\n",
    "\n",
    "data_healthy_vert = dataReader(healthy_vert)\n",
    "data_healthy_hori = dataReader(healthy_hori)\n",
    "data_unhealthy_vert = dataReader(unhealthy_vert)\n",
    "data_unhealthy_hori = dataReader(unhealthy_hori)\n",
    "\n",
    "print(\"starting size: \")\n",
    "print(data_healthy_vert.shape)\n",
    "print(data_healthy_hori.shape)\n",
    "print(data_unhealthy_vert.shape)\n",
    "print(data_unhealthy_hori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "def normalise(df):\n",
    "    df_normalized = df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    return df_normalized\n",
    "\n",
    "data_healthy_hori_norm = normalise(data_healthy_hori)\n",
    "data_healthy_vert_norm = normalise(data_healthy_vert)\n",
    "data_unhealthy_hori_norm = normalise(data_unhealthy_hori)\n",
    "data_unhealthy_vert_norm = normalise(data_unhealthy_vert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checking if data is loaded in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4900690 entries, 0 to 245128\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   x       float64\n",
      " 1   y       float64\n",
      " 2   z       float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 149.6 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4917380 entries, 0 to 245073\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   x       float64\n",
      " 1   y       float64\n",
      " 2   z       float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 150.1 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4943184 entries, 0 to 243985\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   x       float64\n",
      " 1   y       float64\n",
      " 2   z       float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 150.9 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4940044 entries, 0 to 244150\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   x       float64\n",
      " 1   y       float64\n",
      " 2   z       float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 150.8 MB\n",
      "None\n",
      "          x         y         z\n",
      "0  1.518972 -0.273621 -1.237399\n",
      "1  0.426344  1.032882 -0.492941\n",
      "2 -1.461718  0.205159 -0.664739\n",
      "3 -1.461718  0.014458  0.111533\n",
      "4 -2.309598  1.686134  0.582387\n",
      "          x         y         z\n",
      "0 -0.305437 -1.455221 -2.336597\n",
      "1 -0.933473  0.301131  0.738155\n",
      "2  0.012659 -0.149353  0.220816\n",
      "3 -0.028123 -0.154716  0.045115\n",
      "4 -0.794816  0.815970  1.128599\n",
      "          x         y         z\n",
      "0  0.585601  0.092045 -1.400353\n",
      "1  0.633975  1.588867 -1.643453\n",
      "2 -0.027139  1.931687 -1.229282\n",
      "3 -1.171993 -0.429429 -0.472968\n",
      "4 -1.171993  0.188614 -1.805521\n",
      "          x         y         z\n",
      "0  2.009276 -1.462646 -2.633894\n",
      "1  0.838799  0.748395  0.023719\n",
      "2  0.469175  0.799814 -0.918981\n",
      "3 -0.431784  0.894743  0.826018\n",
      "4 -0.393281  0.791904  1.247224\n"
     ]
    }
   ],
   "source": [
    "print(data_healthy_hori_norm.info())\n",
    "print(data_healthy_vert_norm.info())\n",
    "print(data_unhealthy_hori_norm.info())\n",
    "print(data_unhealthy_vert_norm.info())\n",
    "\n",
    "print(data_healthy_hori_norm.head())\n",
    "print(data_healthy_vert_norm.head())\n",
    "print(data_unhealthy_hori_norm.head())\n",
    "print(data_unhealthy_vert_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Downsampling to reduce size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampler(data, a, b):\n",
    "    \"\"\"\n",
    "    data = data\n",
    "    a = start index\n",
    "    b = sampling rate\n",
    "    \"\"\"\n",
    "    x = b\n",
    "    downsampled_data = [data.iloc[a:b,:].sum()/x for i in range(int(len(data)/x))]\n",
    "    return pd.DataFrame(downsampled_data)\n",
    "\n",
    "# Create donwsampled datasets\n",
    "data_healthy_hori_norm_downsampled = downSampler(data_healthy_hori_norm, 0, 500)\n",
    "data_healthy_vert_norm_downsampled = downSampler(data_healthy_vert_norm, 0, 500)\n",
    "data_unhealthy_hori_norm_downsampled = downSampler(data_unhealthy_hori_norm, 0, 500)\n",
    "data_unhealthy_vert_norm_downsampled = downSampler(data_unhealthy_vert_norm, 0, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Checking that data is downsampled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9801, 3)\n",
      "(9834, 3)\n",
      "(9886, 3)\n",
      "(9880, 3)\n"
     ]
    }
   ],
   "source": [
    "print(data_healthy_hori_norm_downsampled.shape)\n",
    "print(data_healthy_vert_norm_downsampled.shape)\n",
    "print(data_unhealthy_hori_norm_downsampled.shape)\n",
    "print(data_unhealthy_vert_norm_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data processing. FFTConolve method is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def FFTConvolve(data):\n",
    "    autocorr = signal.fftconvolve(data,data[::-1],mode='full')\n",
    "    return pd.DataFrame(autocorr)\n",
    "\n",
    "# Create FFTConvolved datasets\n",
    "data_healthy_hori_norm_fftconvole = FFTConvolve(data_healthy_hori_norm_downsampled)\n",
    "data_healthy_vert_norm_fftconvole = FFTConvolve(data_healthy_vert_norm_downsampled)\n",
    "data_unhealthy_hori_norm_fftconvole = FFTConvolve(data_unhealthy_hori_norm_downsampled)\n",
    "data_unhealthy_vert_norm_fftconvole = FFTConvolve(data_unhealthy_vert_norm_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Checking that the data processing step is done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19601, 5)\n",
      "(19667, 5)\n",
      "(19771, 5)\n",
      "(19759, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data_healthy_hori_norm_fftconvole.shape)\n",
    "print(data_healthy_vert_norm_fftconvole.shape)\n",
    "print(data_unhealthy_hori_norm_fftconvole.shape)\n",
    "print(data_unhealthy_vert_norm_fftconvole.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  0\n",
      "1  0\n",
      "2  0\n",
      "3  0\n",
      "4  0\n",
      "       0\n",
      "19766  1\n",
      "19767  1\n",
      "19768  1\n",
      "19769  1\n",
      "19770  1\n",
      "   0\n",
      "0  0\n",
      "1  0\n",
      "2  0\n",
      "3  0\n",
      "4  0\n",
      "       0\n",
      "19754  1\n",
      "19755  1\n",
      "19756  1\n",
      "19757  1\n",
      "19758  1\n"
     ]
    }
   ],
   "source": [
    "# Setting up labels for horiztonal and vertical data\n",
    "y_hori_healthy = pd.DataFrame(np.zeros(int(len(data_healthy_hori_norm_fftconvole)),dtype=int))\n",
    "y_hori_unhealthy = pd.DataFrame(np.ones(int(len(data_unhealthy_hori_norm_fftconvole)),dtype=int))\n",
    "y_vert_healthy = pd.DataFrame(np.zeros(int(len(data_healthy_vert_norm_fftconvole)),dtype=int))\n",
    "y_vert_unhealthy = pd.DataFrame(np.ones(int(len(data_unhealthy_vert_norm_fftconvole)),dtype=int))\n",
    "\n",
    "y_hori = pd.concat([y_hori_healthy, y_hori_unhealthy],axis=0)\n",
    "y_vert = pd.concat([y_vert_healthy, y_vert_unhealthy],axis=0)\n",
    "\n",
    "print(y_hori.head())\n",
    "print(y_hori.tail())\n",
    "print(y_vert.head())\n",
    "print(y_vert.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Preparing data to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hori = pd.concat([data_healthy_hori_norm_fftconvole, data_unhealthy_hori_norm_fftconvole], ignore_index=True) # Concatenate all the data\n",
    "x_vert = pd.concat([data_healthy_vert_norm_fftconvole, data_unhealthy_vert_norm_fftconvole], ignore_index=True) # Concatenate all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>-0.002921</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006539</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>-0.005843</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010898</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.001323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39367</th>\n",
       "      <td>0.006315</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39368</th>\n",
       "      <td>0.005052</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39369</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.006078</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39370</th>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39371</th>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4\n",
       "0      0.002180  0.000712 -0.001461 -0.000248  0.000265\n",
       "1      0.004359  0.001425 -0.002921 -0.000497  0.000529\n",
       "2      0.006539  0.002137 -0.004382 -0.000745  0.000794\n",
       "3      0.008718  0.002850 -0.005843 -0.000993  0.001059\n",
       "4      0.010898  0.003562 -0.007304 -0.001241  0.001323\n",
       "...         ...       ...       ...       ...       ...\n",
       "39367  0.006315  0.010130  0.005048  0.000791  0.000039\n",
       "39368  0.005052  0.008104  0.004039  0.000633  0.000031\n",
       "39369  0.003789  0.006078  0.003029  0.000475  0.000023\n",
       "39370  0.002526  0.004052  0.002019  0.000316  0.000015\n",
       "39371  0.001263  0.002026  0.001010  0.000158  0.000008\n",
       "\n",
       "[39372 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hori # Check if data is concatenated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000356</td>\n",
       "      <td>-0.000585</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001067</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.002339</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001778</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39421</th>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.012177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.009742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39423</th>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.007306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39424</th>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.004871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39425</th>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39426 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4\n",
       "0      0.000356 -0.000585 -0.000008  0.000204  0.000043\n",
       "1      0.000711 -0.001169 -0.000016  0.000408  0.000087\n",
       "2      0.001067 -0.001754 -0.000023  0.000612  0.000130\n",
       "3      0.001422 -0.002339 -0.000031  0.000816  0.000173\n",
       "4      0.001778 -0.002923 -0.000039  0.001020  0.000216\n",
       "...         ...       ...       ...       ...       ...\n",
       "39421  0.004556  0.003275  0.015486  0.005355  0.012177\n",
       "39422  0.003645  0.002620  0.012389  0.004284  0.009742\n",
       "39423  0.002734  0.001965  0.009292  0.003213  0.007306\n",
       "39424  0.001823  0.001310  0.006194  0.002142  0.004871\n",
       "39425  0.000911  0.000655  0.003097  0.001071  0.002435\n",
       "\n",
       "[39426 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vert # Check if data is concatenated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_hori_train, x_hori_test, y_hori_train, y_hori_test = train_test_split(x_hori, y_hori, test_size=0.25, shuffle=True)\n",
    "\n",
    "x_vert_train, x_vert_test, y_vert_train, y_vert_test = train_test_split(x_vert, y_vert, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "237/237 [==============================] - 3s 6ms/step - loss: 0.0471 - accuracy: 0.9907 - val_loss: 0.0063 - val_accuracy: 0.9997\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.0022 - val_accuracy: 0.9997\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 0.9997\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0012 - val_accuracy: 0.9995\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 6.9171e-04 - val_accuracy: 0.9998\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 6.9411e-04 - val_accuracy: 0.9997\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 8.7912e-04 - accuracy: 0.9997 - val_loss: 6.0481e-04 - val_accuracy: 0.9998\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 7.4702e-04 - accuracy: 0.9997 - val_loss: 5.7607e-04 - val_accuracy: 0.9997\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 6.9109e-04 - accuracy: 0.9996 - val_loss: 6.6840e-04 - val_accuracy: 0.9997\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 7.1662e-04 - accuracy: 0.9997 - val_loss: 3.9972e-04 - val_accuracy: 0.9998\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 7.2697e-04 - accuracy: 0.9996 - val_loss: 6.4249e-04 - val_accuracy: 0.9997\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 6.3794e-04 - accuracy: 0.9998 - val_loss: 7.9433e-04 - val_accuracy: 0.9995\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 5.4696e-04 - accuracy: 0.9997 - val_loss: 3.8921e-04 - val_accuracy: 0.9998\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 5.7630e-04 - accuracy: 0.9997 - val_loss: 3.8179e-04 - val_accuracy: 0.9998\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 4.8111e-04 - accuracy: 0.9997 - val_loss: 2.5192e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 4.8088e-04 - accuracy: 0.9999 - val_loss: 2.6205e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 4.7979e-04 - accuracy: 0.9997 - val_loss: 2.1518e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 5.4124e-04 - accuracy: 0.9998 - val_loss: 2.1807e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.4704e-04 - accuracy: 0.9998 - val_loss: 5.9838e-04 - val_accuracy: 0.9997\n",
      "Epoch 21/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 4.0747e-04 - accuracy: 0.9997 - val_loss: 1.8229e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 3.3775e-04 - accuracy: 0.9999 - val_loss: 3.4824e-04 - val_accuracy: 0.9997\n",
      "Epoch 23/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.5695e-04 - accuracy: 0.9998 - val_loss: 2.0211e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.5895e-04 - accuracy: 0.9999 - val_loss: 2.4698e-04 - val_accuracy: 0.9998\n",
      "Epoch 25/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 3.9991e-04 - accuracy: 0.9997 - val_loss: 3.0181e-04 - val_accuracy: 0.9998\n",
      "Epoch 26/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.8054e-04 - accuracy: 0.9999 - val_loss: 1.2722e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 2.7397e-04 - accuracy: 0.9999 - val_loss: 4.0971e-04 - val_accuracy: 0.9997\n",
      "Epoch 28/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.1818e-04 - accuracy: 0.9999 - val_loss: 1.5760e-04 - val_accuracy: 0.9998\n",
      "Epoch 29/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 4.9878e-04 - accuracy: 0.9998 - val_loss: 1.0076e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 2.6895e-04 - accuracy: 0.9999 - val_loss: 9.1131e-05 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 2.4189e-04 - accuracy: 0.9999 - val_loss: 1.8171e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 1.7080e-04 - accuracy: 1.0000 - val_loss: 3.2162e-04 - val_accuracy: 0.9997\n",
      "Epoch 33/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 2.9767e-04 - accuracy: 0.9999 - val_loss: 3.3941e-04 - val_accuracy: 0.9997\n",
      "Epoch 34/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 2.7647e-04 - accuracy: 0.9999 - val_loss: 4.6589e-04 - val_accuracy: 0.9997\n",
      "Epoch 35/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 4.3709e-04 - accuracy: 0.9997 - val_loss: 6.1406e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.6322e-04 - accuracy: 0.9998 - val_loss: 1.2772e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 1.8823e-04 - accuracy: 1.0000 - val_loss: 6.0847e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 2.3949e-04 - accuracy: 0.9999 - val_loss: 4.6735e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 1.4389e-04 - accuracy: 1.0000 - val_loss: 1.3723e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 1.8682e-04 - accuracy: 1.0000 - val_loss: 5.5279e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.2119e-04 - accuracy: 0.9999 - val_loss: 3.2430e-04 - val_accuracy: 0.9998\n",
      "Epoch 42/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 1.0965e-04 - accuracy: 1.0000 - val_loss: 1.4615e-04 - val_accuracy: 0.9998\n",
      "Epoch 43/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 4.9551e-04 - accuracy: 0.9998 - val_loss: 1.9443e-04 - val_accuracy: 0.9998\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 32)                192       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4450 (17.38 KB)\n",
      "Trainable params: 4450 (17.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\jared\\AppData\\Local\\Temp\\tmpc263hret\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jared\\AppData\\Local\\Temp\\tmpc263hret\\assets\n",
      "c:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "237/237 [==============================] - 2s 4ms/step - loss: 0.0461 - accuracy: 0.9948 - val_loss: 0.0075 - val_accuracy: 0.9976\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0021 - val_accuracy: 0.9993\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0015 - val_accuracy: 0.9997\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 9.8915e-04 - accuracy: 0.9997 - val_loss: 0.0017 - val_accuracy: 0.9998\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 8.8056e-04 - accuracy: 0.9997 - val_loss: 0.0011 - val_accuracy: 0.9995\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 7.6915e-04 - accuracy: 0.9997 - val_loss: 8.3697e-04 - val_accuracy: 0.9997\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 6.6091e-04 - accuracy: 0.9998 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 5.3164e-04 - accuracy: 0.9998 - val_loss: 6.6499e-04 - val_accuracy: 0.9998\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 4.8598e-04 - accuracy: 0.9998 - val_loss: 5.8428e-04 - val_accuracy: 0.9998\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 4.2968e-04 - accuracy: 0.9999 - val_loss: 8.7405e-04 - val_accuracy: 0.9997\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 3.6294e-04 - accuracy: 0.9999 - val_loss: 4.8592e-04 - val_accuracy: 0.9998\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 3.5241e-04 - accuracy: 0.9999 - val_loss: 4.4487e-04 - val_accuracy: 0.9998\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 1s 5ms/step - loss: 3.7742e-04 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 0.9998\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.0322e-04 - accuracy: 0.9999 - val_loss: 5.9550e-04 - val_accuracy: 0.9997\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 3.2142e-04 - accuracy: 0.9999 - val_loss: 4.5034e-04 - val_accuracy: 0.9998\n",
      "Epoch 17/50\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 2.7872e-04 - accuracy: 0.9999 - val_loss: 0.0011 - val_accuracy: 0.9998\n",
      "Epoch 18/50\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 2.8253e-04 - accuracy: 0.9999 - val_loss: 6.3961e-04 - val_accuracy: 0.9997\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 32)                192       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4450 (17.38 KB)\n",
      "Trainable params: 4450 (17.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\jared\\AppData\\Local\\Temp\\tmpx1i4mon0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jared\\AppData\\Local\\Temp\\tmpx1i4mon0\\assets\n",
      "c:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20404"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode=\"auto\")\n",
    "\n",
    "def representative_dataset_hori():\n",
    "    for val in x_hori_test:\n",
    "        yield [np.array(val, dtype=np.float32)]\n",
    "\n",
    "def representative_dataset_vert():\n",
    "    for val in x_vert_test:\n",
    "        yield [np.array(val, dtype=np.float32)]\n",
    "\n",
    "def get_model(x_train, y_train, epochs=10, validation_split=0.2, batch_size=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(5,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax')) # Output layer needs to correspond to the number of classes for softmax\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size, callbacks=[early_stopping])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "FFTmodel_hori = get_model(x_hori_train, y_hori_train, epochs=50, validation_split=0.2, batch_size=100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel_hori)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_hori\n",
    "tflite_model_hori = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"FFT_model_hori_quantized.tflite\", \"wb\").write(tflite_model_hori)\n",
    "\n",
    "FFTmodel_vert = get_model(x_vert_train, y_vert_train, epochs=50, validation_split=0.2, batch_size=100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(FFTmodel_vert)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_vert\n",
    "tflite_model_vert = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"FFT_model_vert_quantized.tflite\", \"wb\").write(tflite_model_vert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Converting of model to C array (Run below line on bash)\n",
    "\n",
    "`xxd -i FFT_model_fullint_quantized.tflite > FFT_model_fullint_quantized.cc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Baseline test accuracy: 100.0\n",
      "The Baseline test accuracy: 99.95942115783691\n"
     ]
    }
   ],
   "source": [
    "# Testing the baseline model on the test dataset.\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "_, baseline_hori_model_accuracy = FFTmodel_hori.evaluate(x=x_hori_test, y=y_hori_test, verbose=0)\n",
    "\n",
    "# Printing the baseline test accuracy in percentage.\n",
    "print('The Baseline test accuracy:', baseline_hori_model_accuracy * 100)\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "_, baseline_vert_model_accuracy = FFTmodel_vert.evaluate(x=x_vert_test, y=y_vert_test, verbose=0)\n",
    "\n",
    "# Printing the baseline test accuracy in percentage.\n",
    "print('The Baseline test accuracy:', baseline_vert_model_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hori Quantized TFLite Model Test Accuracy: 100.0\n",
      "Baseline Hori Keras Model Test Accuracy: 100.0\n",
      "Vert Quantized TFLite Model Test Accuracy: 99.9594197017348\n",
      "Baseline Vert Keras Model Test Accuracy: 99.95942115783691\n"
     ]
    }
   ],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter, x_test, y_test):\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    num_test_samples = len(x_test)\n",
    "\n",
    "    # Run predictions on every set in the \"test\" dataset.\n",
    "    prediction_y = []\n",
    "    for i in range(num_test_samples):\n",
    "\n",
    "        # Pre-processing the data to fit it with the model's input.\n",
    "        input_data = np.array(x_test.iloc[i,:], dtype=np.float32)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        prediction_y.append(output_data.argmax())\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_y)):\n",
    "        if prediction_y[index] == y_test.iloc[index][0]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_y)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Passing the FP-16 TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('FFT_model_hori_quantized.tflite')\n",
    "\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy_hori = evaluate_model(interpreter, x_hori_test, y_hori_test)\n",
    "\n",
    "# Printing the test accuracy for the FP-16 quantized TFLite model and the baseline Keras model.\n",
    "print('Hori Quantized TFLite Model Test Accuracy:', test_accuracy_hori*100)\n",
    "\n",
    "# Printing the test accuracy for the baseline Keras model.\n",
    "print('Baseline Hori Keras Model Test Accuracy:', baseline_hori_model_accuracy*100)\n",
    "\n",
    "# Testing the full integer quantized model on the test dataset.\n",
    "\n",
    "# Passing the full integer quantized TF Lite model to the interpreter.\n",
    "interpreter = tf.lite.Interpreter('FFT_model_vert_quantized.tflite')\n",
    "\n",
    "# Allocating tensors.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Evaluating the model on the test dataset.\n",
    "test_accuracy_vert = evaluate_model(interpreter, x_vert_test, y_vert_test)\n",
    "\n",
    "# Printing the test accuracy for the full integer quantized TFLite model and the baseline Keras model.\n",
    "print('Vert Quantized TFLite Model Test Accuracy:', test_accuracy_vert*100)\n",
    "\n",
    "# Printing the test accuracy for the baseline Keras model.\n",
    "print('Baseline Vert Keras Model Test Accuracy:', baseline_vert_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current to-dos:\n",
    "* Find way to build FFTConvolve model without overfitting\n",
    "* Find way to do hyperparameters testing for models\n",
    "* Create template to test TF model and TFLite models quickly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
